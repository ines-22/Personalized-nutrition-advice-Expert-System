# University Chatbot
This project is a document retrieval and question-answering system designed to provide detailed answers based on documents stored in a directory. It uses various tools and libraries, including **LangChain**, **Ollama**, and **ChromaDB**, to extract, process, and query information from documents related to the **Institut Sup√©rieur d'Informatique** (ISI). 

## Installation

1. **Mount Google Drive** to access files stored there:
    ```python
    from google.colab import drive
    drive.mount('/content/drive')
    ```

2. **Install Required Libraries**:
    ```bash
    !pip install -q colab-xterm PyPDF2 xlrd pypdf pandas python-docx langchain docx2txt transformers ollama chromadb langchain-ollama openpyxl
    !pip install --upgrade --quiet langchain-community unstructured openpyxl
    ```

3. **Pull LLM Model** (Ollama):
    ```bash
    !ollama pull llama3.1
    ```

## Usage

1. **Document Loading**: 
    - Specify the directory to load documents from:
    ```python
    parent_directory = '/content/drive/MyDrive/Dossier_Accreditation CTI_ISI_2024__'
    documents = load_documents_from_directory(parent_directory)
    ```

2. **Preprocess Text**: 
    - Clean the text before storing it in the vectorstore:
    ```python
    documents = preprocess_text(documents)
    ```

3. **Create Vector Store**:
    - Generate embeddings and store them in ChromaDB:
    ```python
    vectorstore = create_vectorstore(documents, embedding_model)
    ```

4. **Querying the System**:
    - Use `query_with_context` to get answers based on your question:
    ```python
    question = "C'est qui Mr Salah Salhi?"
    answer = query_with_context(question)
    print(answer)
    ```

## Functions

### `load_documents_from_directory(directory)`
- Loads documents from specified directory, filtering for supported file types.

### `load_and_extract_text(file_path)`
- Extracts text based on file type using respective libraries.

### `preprocess_text(text_list)`
- Preprocesses extracted text, cleaning it for optimal performance in embeddings and querying.

### `create_vectorstore(documents, embedding_model, chunk_size=1000, chunk_overlap=100)`
- Creates and stores document embeddings in a ChromaDB vectorstore with chunking.

### `query_with_context(question, top_k=5)`
- Retrieves top matching document contexts and queries the LLM to answer the provided question.

## Dependencies

- [LangChain](https://github.com/hwchase17/langchain)
- [Ollama](https://ollama.com/)
- [ChromaDB](https://www.trychroma.com/)
